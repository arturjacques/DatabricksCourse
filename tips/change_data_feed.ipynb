{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d52ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install delta-spark==3.3.2\n",
    "#pip install pyspark==3.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ba9967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:42:36 WARN Utils: Your hostname, jacques-work resolves to a loopback address: 127.0.1.1; using 192.168.1.23 instead (on interface wlp0s20f3)\n",
      "25/08/12 18:42:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/jacques/Documents/courses/DatabricksCourse/venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jacques/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jacques/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-16b86063-7d73-4163-8e0f-3a6a5756186e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 133ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-16b86063-7d73-4163-8e0f-3a6a5756186e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "25/08/12 18:42:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"deltaLakeApp\")\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f7ea4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0319e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"tmp/checkpoint\"\n",
    "\n",
    "orders_table = \"orders\"\n",
    "orders_qa_table = \"orders_qa\"\n",
    "orders_analyse = \"orders_analyse\"\n",
    "streaming_example = \"stream_table\"\n",
    "orders_qa_checkpoint = f\"{checkpoint_path}/{orders_qa_table}\"\n",
    "orders_analyse_checkpoint = f\"{checkpoint_path}/{orders_analyse}\"\n",
    "streaming_example_checkpoint = f\"{checkpoint_path}/{streaming_example}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdaef900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {orders_table}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {orders_qa_table}\")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "  shutil.rmtree(checkpoint_path)\n",
    "\n",
    "if os.path.exists(\"spark-warehouse\"):\n",
    "  shutil.rmtree(\"spark-warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317bb784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:33:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "          CREATE OR REPLACE TABLE {orders_table} (\n",
    "            order_id STRING COMMENT 'Unique identifier for the order',\n",
    "            client_id STRING COMMENT 'Identifier of the client who placed the order',\n",
    "            order_value DECIMAL(18, 2) COMMENT 'Total value of the order in the local currency',\n",
    "            order_date TIMESTAMP COMMENT 'Timestamp when the order was placed',\n",
    "            country_code STRING COMMENT 'Country in ISO 3166 Alpha-2 code'\n",
    "          )\n",
    "          USING DELTA\n",
    "          TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "          COMMENT 'orders table'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6611621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------+----------------------------------------------+\n",
      "|col_name                    |data_type                                                                          |comment                                       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+----------------------------------------------+\n",
      "|order_id                    |string                                                                             |Unique identifier for the order               |\n",
      "|client_id                   |string                                                                             |Identifier of the client who placed the order |\n",
      "|order_value                 |decimal(18,2)                                                                      |Total value of the order in the local currency|\n",
      "|order_date                  |timestamp                                                                          |Timestamp when the order was placed           |\n",
      "|country_code                |string                                                                             |Country in ISO 3166 Alpha-2 code              |\n",
      "|                            |                                                                                   |                                              |\n",
      "|# Detailed Table Information|                                                                                   |                                              |\n",
      "|Name                        |spark_catalog.default.orders                                                       |                                              |\n",
      "|Type                        |MANAGED                                                                            |                                              |\n",
      "|Comment                     |orders table                                                                       |                                              |\n",
      "|Location                    |file:/home/jacques/Documents/courses/DatabricksCourse/tips/spark-warehouse/orders  |                                              |\n",
      "|Provider                    |delta                                                                              |                                              |\n",
      "|Table Properties            |[delta.enableChangeDataFeed=true,delta.minReaderVersion=1,delta.minWriterVersion=4]|                                              |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {orders_table}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4121731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, TimestampType\n",
    "\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"client_id\", StringType(), True),\n",
    "    StructField(\"order_value\", DecimalType(18, 2), True),\n",
    "    StructField(\"order_date\", TimestampType(), True),\n",
    "    StructField(\"country_code\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"ORD001\", None, Decimal(\"125.50\"), datetime(2025, 8, 1, 10, 30), \"BR\"),\n",
    "    (\"ORD002\", \"CL002\", Decimal(\"299.99\"), datetime(2025, 8, 1, 11, 15), \"BR\"),\n",
    "    (\"ORD003\", \"CL003\", Decimal(\"75.00\"), datetime(2025, 8, 2, 9, 45), \"BR\"),\n",
    "    (\"ORD004\", \"CL004\", Decimal(\"560.00\"), datetime(2025, 8, 2, 15, 10), \"BR\"),\n",
    "    (\"ORD005\", \"CL001\", Decimal(\"130.25\"), datetime(2025, 8, 3, 14, 0), \"BR\"),\n",
    "    (\"ORD006\", \"CL005\", Decimal(\"199.99\"), datetime(2025, 8, 3, 16, 30), \"US\"),\n",
    "    (\"ORD007\", \"CL006\", Decimal(\"89.95\"), datetime(2025, 8, 4, 12, 45), \"US\"),\n",
    "    (\"ORD008\", \"CL002\", Decimal(\"149.00\"), datetime(2025, 8, 4, 17, 25), \"US\"),\n",
    "    (\"ORD009\", \"CL007\", Decimal(\"210.75\"), datetime(2025, 8, 4, 18, 50), \"US\"),\n",
    "    (\"ORD010\", \"CL008\", Decimal(\"55.55\"), datetime(2025, 8, 4, 19, 5), \"US\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "orders_df = spark.createDataFrame(data, schema=order_schema)\n",
    "\n",
    "# Write to Delta table\n",
    "orders_df.coalesce(1).write.format(\"delta\").mode(\"append\").saveAsTable(orders_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb34353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+------------+------------+---------------+--------------------+\n",
      "|order_id|client_id|order_value|         order_date|country_code|_change_type|_commit_version|   _commit_timestamp|\n",
      "+--------+---------+-----------+-------------------+------------+------------+---------------+--------------------+\n",
      "|  ORD001|     NULL|     125.50|2025-08-01 10:30:00|          BR|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD002|    CL002|     299.99|2025-08-01 11:15:00|          BR|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      75.00|2025-08-02 09:45:00|          BR|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD004|    CL004|     560.00|2025-08-02 15:10:00|          BR|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD005|    CL001|     130.25|2025-08-03 14:00:00|          BR|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD006|    CL005|     199.99|2025-08-03 16:30:00|          US|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD007|    CL006|      89.95|2025-08-04 12:45:00|          US|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD008|    CL002|     149.00|2025-08-04 17:25:00|          US|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD009|    CL007|     210.75|2025-08-04 18:50:00|          US|      insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD010|    CL008|      55.55|2025-08-04 19:05:00|          US|      insert|              1|2025-08-12 18:33:...|\n",
      "+--------+---------+-----------+-------------------+------------+------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 0) \\\n",
    "  .option(\"endingVersion\", 1) \\\n",
    "  .table(orders_table)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c636d4",
   "metadata": {},
   "source": [
    "## Changing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f4af7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DELETE FROM {orders_table} WHERE order_id='ORD006'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c146320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "upsert_data = [\n",
    "    (\"ORD003\", \"CL003\", Decimal(\"80.00\"), datetime(2025, 8, 5, 9, 0), \"BR\"),   # Update\n",
    "    (\"ORD006\", \"CL005\", Decimal(\"215.00\"), datetime(2025, 8, 5, 10, 0), \"BR\"), # Update\n",
    "    (\"ORD011\", \"CL009\", Decimal(\"99.99\"), datetime(2025, 8, 5, 11, 30), \"US\"), # New\n",
    "    (\"ORD012\", \"CL010\", Decimal(\"150.00\"), datetime(2025, 8, 5, 12, 45), \"US\"),# New\n",
    "    (\"ORD013\", \"CL011\", Decimal(\"300.00\"), datetime(2025, 8, 5, 13, 15), \"BR\") # New\n",
    "]\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, orders_table)\n",
    "upsert_df = spark.createDataFrame(upsert_data, order_schema)\n",
    "\n",
    "delta_table.alias(\"target\") \\\n",
    "    .merge(upsert_df.alias(\"source\"), \"target.order_id = source.order_id\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02a53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|order_id|client_id|order_value|         order_date|country_code|    _change_type|_commit_version|   _commit_timestamp|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|  ORD006|    CL005|     199.99|2025-08-03 16:30:00|          US|          delete|              2|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      75.00|2025-08-02 09:45:00|          BR| update_preimage|              3|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      80.00|2025-08-05 09:00:00|          BR|update_postimage|              3|2025-08-12 18:33:...|\n",
      "|  ORD006|    CL005|     215.00|2025-08-05 10:00:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD011|    CL009|      99.99|2025-08-05 11:30:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD012|    CL010|     150.00|2025-08-05 12:45:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD013|    CL011|     300.00|2025-08-05 13:15:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 2) \\\n",
    "  .option(\"endingVersion\", 4) \\\n",
    "  .table(orders_table)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ca19a",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "658eb3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:35:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "streaming_process = (spark\n",
    "  .readStream\n",
    "  .option(\"readChangeFeed\", \"true\")\n",
    "  .table(orders_table)\n",
    "  .writeStream\n",
    "  .format('delta')\n",
    "  .option(\"checkpointLocation\", streaming_example_checkpoint)\n",
    "  .trigger(availableNow=True)\n",
    "  .outputMode(\"append\")\n",
    ")\n",
    "\n",
    "streaming_process.toTable(streaming_example).awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a20c7ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+------------+------------+---------------+--------------------+\n",
      "|order_id|client_id|order_value|         order_date|country_code|_change_type|_commit_version|   _commit_timestamp|\n",
      "+--------+---------+-----------+-------------------+------------+------------+---------------+--------------------+\n",
      "|  ORD001|     NULL|     125.50|2025-08-01 10:30:00|          BR|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD002|    CL002|     299.99|2025-08-01 11:15:00|          BR|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      80.00|2025-08-05 09:00:00|          BR|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD004|    CL004|     560.00|2025-08-02 15:10:00|          BR|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD005|    CL001|     130.25|2025-08-03 14:00:00|          BR|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD006|    CL005|     215.00|2025-08-05 10:00:00|          BR|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD007|    CL006|      89.95|2025-08-04 12:45:00|          US|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD008|    CL002|     149.00|2025-08-04 17:25:00|          US|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD009|    CL007|     210.75|2025-08-04 18:50:00|          US|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD010|    CL008|      55.55|2025-08-04 19:05:00|          US|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD011|    CL009|      99.99|2025-08-05 11:30:00|          US|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD012|    CL010|     150.00|2025-08-05 12:45:00|          US|      insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD013|    CL011|     300.00|2025-08-05 13:15:00|          BR|      insert|              3|2025-08-12 18:33:...|\n",
      "+--------+---------+-----------+-------------------+------------+------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(streaming_example).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700526cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x783c846e9400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"UPDATE {orders_table} SET order_value=100.00 WHERE order_id='ORD007'\")\n",
    "streaming_process.toTable(streaming_example).awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f437093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|order_id|client_id|order_value|         order_date|country_code|    _change_type|_commit_version|   _commit_timestamp|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|  ORD001|     NULL|     125.50|2025-08-01 10:30:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD002|    CL002|     299.99|2025-08-01 11:15:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      80.00|2025-08-05 09:00:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD004|    CL004|     560.00|2025-08-02 15:10:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD005|    CL001|     130.25|2025-08-03 14:00:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD006|    CL005|     215.00|2025-08-05 10:00:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD007|    CL006|      89.95|2025-08-04 12:45:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD008|    CL002|     149.00|2025-08-04 17:25:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD009|    CL007|     210.75|2025-08-04 18:50:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD010|    CL008|      55.55|2025-08-04 19:05:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD011|    CL009|      99.99|2025-08-05 11:30:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD012|    CL010|     150.00|2025-08-05 12:45:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD013|    CL011|     300.00|2025-08-05 13:15:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD007|    CL006|      89.95|2025-08-04 12:45:00|          US| update_preimage|              4|2025-08-12 18:33:...|\n",
      "|  ORD007|    CL006|     100.00|2025-08-04 12:45:00|          US|update_postimage|              4|2025-08-12 18:33:...|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(streaming_example).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749bccb",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5091bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|order_id|client_id|order_value|         order_date|country_code|    _change_type|_commit_version|   _commit_timestamp|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|  ORD006|    CL005|     199.99|2025-08-03 16:30:00|          US|          delete|              2|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      75.00|2025-08-02 09:45:00|          BR| update_preimage|              3|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      80.00|2025-08-05 09:00:00|          BR|update_postimage|              3|2025-08-12 18:33:...|\n",
      "|  ORD006|    CL005|     215.00|2025-08-05 10:00:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD011|    CL009|      99.99|2025-08-05 11:30:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD012|    CL010|     150.00|2025-08-05 12:45:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD013|    CL011|     300.00|2025-08-05 13:15:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD001|     NULL|     125.50|2025-08-01 10:30:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD002|    CL002|     299.99|2025-08-01 11:15:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      75.00|2025-08-02 09:45:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD004|    CL004|     560.00|2025-08-02 15:10:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD005|    CL001|     130.25|2025-08-03 14:00:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD006|    CL005|     199.99|2025-08-03 16:30:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD007|    CL006|      89.95|2025-08-04 12:45:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD008|    CL002|     149.00|2025-08-04 17:25:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD009|    CL007|     210.75|2025-08-04 18:50:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD010|    CL008|      55.55|2025-08-04 19:05:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 0) \\\n",
    "  .option(\"endingVersion\", 3) \\\n",
    "  .table(orders_table)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e0a00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, isnull, col, lit, sum, date_format\n",
    "\n",
    "\n",
    "def change_data_feed_column_signal(df, operation, column_name, column_analised):\n",
    "    return (df\n",
    "    .withColumn(column_name, \n",
    "                when(col(\"_change_type\") == 'delete', operation(column_analised)*lit(-1))\n",
    "                .when(col(\"_change_type\") == 'update_preimage', operation(column_analised)*lit(-1))\n",
    "                .when(col(\"_change_type\") == 'update_postimage', operation(column_analised))\n",
    "                .when(col(\"_change_type\") == 'insert', operation(column_analised))\n",
    "            ))\n",
    "\n",
    "def is_null_signal(column):\n",
    "  return isnull(column).cast('int')\n",
    "\n",
    "def lit_1(column):\n",
    "  return lit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6343e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "|order_id_null_count|client_id_null_count|order_value_null_count|order_date_null_count|country_code_null_count|number_of_rows|order_total_value|table_source_name|\n",
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "|                  0|                   1|                     0|                    0|                      0|            13|          2465.98|           orders|\n",
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = df.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\").columns\n",
    "\n",
    "df_changed = df\n",
    "select_expr = list()\n",
    "\n",
    "for column in columns:\n",
    "  df_changed = change_data_feed_column_signal(df_changed, is_null_signal, f\"{column}_signal\", column)\n",
    "  select_expr.append(f\"sum({column}_signal) as {column}_null_count\")\n",
    "\n",
    "df_changed = change_data_feed_column_signal(df_changed, lit_1, \"row_value_count\", \"*\")\n",
    "select_expr.append(f\"sum(row_value_count) number_of_rows\")\n",
    "\n",
    "df_changed = change_data_feed_column_signal(df_changed, col, \"order_value_sum\", \"order_value\")\n",
    "select_expr.append(f\"sum(order_value_sum) order_total_value\")\n",
    "\n",
    "df_selected = df_changed.selectExpr(*select_expr)\n",
    "\n",
    "df_with_table_name = df_selected.withColumn(\"table_source_name\", lit(orders_table))\n",
    "\n",
    "df_with_table_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e502d",
   "metadata": {},
   "source": [
    "## Streaming Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaaeb9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:34:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "def _write_table_update(columns):\n",
    "  column_set = dict()\n",
    "  for column in columns:\n",
    "    column_set[column] = f\"source.{column} + target.{column}\"\n",
    "  return column_set\n",
    "\n",
    "def process_table(batch_df, batch_id):\n",
    "  df_changed = batch_df\n",
    "  columns = batch_df.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\").columns\n",
    "  select_expr = list()\n",
    "\n",
    "  for column in columns:\n",
    "    df_changed = change_data_feed_column_signal(df_changed, is_null_signal, f\"{column}_signal\", column)\n",
    "    select_expr.append(f\"sum({column}_signal) as {column}_null_count\")\n",
    "\n",
    "  df_changed = change_data_feed_column_signal(df_changed, lit_1, \"row_value_count\", \"*\")\n",
    "  select_expr.append(f\"sum(row_value_count) number_of_rows\")\n",
    "\n",
    "  df_changed = change_data_feed_column_signal(df_changed, col, \"order_value_sum\", \"order_value\")\n",
    "  select_expr.append(f\"sum(order_value_sum) order_total_value\")\n",
    "\n",
    "  df_selected = df_changed.selectExpr(*select_expr)\n",
    "\n",
    "  df_with_table_name = df_selected.withColumn(\"table_source_name\", lit(orders_table))\n",
    "\n",
    "  if spark.catalog.tableExists(orders_qa_table):\n",
    "    set_columns = _write_table_update(df_with_table_name.drop(\"table_source_name\").columns)\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, orders_qa_table)\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "      df_with_table_name.alias(\"source\"),\n",
    "          \"target.table_source_name = source.table_source_name\"\n",
    "      ).whenMatchedUpdate(\n",
    "          set = set_columns\n",
    "      ).whenNotMatchedInsertAll().execute()\n",
    "  else:\n",
    "    df_with_table_name.write.format(\"delta\").mode(\"append\").saveAsTable(orders_qa_table)\n",
    "\n",
    "\n",
    "streaming = (spark\n",
    "             .readStream\n",
    "             .option(\"readChangeFeed\", \"true\")\n",
    "             .table(orders_table)\n",
    "             .writeStream\n",
    "             .format('delta')\n",
    "             .option(\"checkpointLocation\", orders_qa_checkpoint)\n",
    "             .foreachBatch(process_table)\n",
    "             .trigger(availableNow=True)             \n",
    ")\n",
    "\n",
    "streaming.start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11d93a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "|order_id_null_count|client_id_null_count|order_value_null_count|order_date_null_count|country_code_null_count|number_of_rows|order_total_value|table_source_name|\n",
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "|                  0|                   1|                     0|                    0|                      0|            13|          2476.03|           orders|\n",
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(orders_qa_table).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c9c7f",
   "metadata": {},
   "source": [
    "## Fixing table null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "878be60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+------------+\n",
      "|order_id|client_id|order_value|         order_date|country_code|\n",
      "+--------+---------+-----------+-------------------+------------+\n",
      "|  ORD001|     NULL|     125.50|2025-08-01 10:30:00|          BR|\n",
      "|  ORD002|    CL002|     299.99|2025-08-01 11:15:00|          BR|\n",
      "|  ORD003|    CL003|      80.00|2025-08-05 09:00:00|          BR|\n",
      "|  ORD004|    CL004|     560.00|2025-08-02 15:10:00|          BR|\n",
      "|  ORD005|    CL001|     130.25|2025-08-03 14:00:00|          BR|\n",
      "|  ORD006|    CL005|     215.00|2025-08-05 10:00:00|          BR|\n",
      "|  ORD007|    CL006|     100.00|2025-08-04 12:45:00|          US|\n",
      "|  ORD008|    CL002|     149.00|2025-08-04 17:25:00|          US|\n",
      "|  ORD009|    CL007|     210.75|2025-08-04 18:50:00|          US|\n",
      "|  ORD010|    CL008|      55.55|2025-08-04 19:05:00|          US|\n",
      "|  ORD011|    CL009|      99.99|2025-08-05 11:30:00|          US|\n",
      "|  ORD012|    CL010|     150.00|2025-08-05 12:45:00|          US|\n",
      "|  ORD013|    CL011|     300.00|2025-08-05 13:15:00|          BR|\n",
      "+--------+---------+-----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(orders_table).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b717b89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"UPDATE {orders_table} SET client_id='CL001' WHERE order_id='ORD001'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b18fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|order_id|client_id|order_value|         order_date|country_code|    _change_type|_commit_version|   _commit_timestamp|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|  ORD007|    CL006|      89.95|2025-08-04 12:45:00|          US| update_preimage|              4|2025-08-12 18:33:...|\n",
      "|  ORD007|    CL006|     100.00|2025-08-04 12:45:00|          US|update_postimage|              4|2025-08-12 18:33:...|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 4) \\\n",
    "  .option(\"endingVersion\", 4) \\\n",
    "  .table(orders_table)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ce1bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:34:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "|order_id_null_count|client_id_null_count|order_value_null_count|order_date_null_count|country_code_null_count|number_of_rows|order_total_value|table_source_name|\n",
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "|                  0|                   0|                     0|                    0|                      0|            13|          2476.03|           orders|\n",
      "+-------------------+--------------------+----------------------+---------------------+-----------------------+--------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.start().awaitTermination()\n",
    "spark.table(orders_qa_table).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044e7ac",
   "metadata": {},
   "source": [
    "## Data Analysis using CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1b217d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 0) \\\n",
    "  .option(\"endingVersion\", 3) \\\n",
    "  .table(orders_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e7cc81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+------------+-----------+\n",
      "|year_month|country_code|client_id|total_orders|total_value|\n",
      "+----------+------------+---------+------------+-----------+\n",
      "|   2025_08|          US|    CL005|           0|       0.00|\n",
      "|   2025_08|          BR|    CL003|           1|      80.00|\n",
      "|   2025_08|          US|    CL009|           1|      99.99|\n",
      "|   2025_08|          US|    CL010|           1|     150.00|\n",
      "|   2025_08|          BR|    CL011|           1|     300.00|\n",
      "|   2025_08|          BR|    CL005|           1|     215.00|\n",
      "|   2025_08|          US|    CL007|           1|     210.75|\n",
      "|   2025_08|          BR|    CL002|           1|     299.99|\n",
      "|   2025_08|          US|    CL002|           1|     149.00|\n",
      "|   2025_08|          US|    CL006|           1|      89.95|\n",
      "|   2025_08|          BR|  IS_NULL|           1|     125.50|\n",
      "|   2025_08|          US|    CL008|           1|      55.55|\n",
      "|   2025_08|          BR|    CL004|           1|     560.00|\n",
      "|   2025_08|          BR|    CL001|           1|     130.25|\n",
      "+----------+------------+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_signal_rule_orders = change_data_feed_column_signal(df, col, \"order_value\", \"order_value\")\n",
    "df_signal_rule_count = change_data_feed_column_signal(df_signal_rule_orders, lit_1, \"any\", \"count_order\")\n",
    "\n",
    "(df_signal_rule_count\n",
    " .withColumn(\"year_month\", date_format(col(\"order_date\"), \"yyyy_MM\"))\n",
    " .groupBy(\"year_month\", \"country_code\", \"client_id\")\n",
    " .agg(sum(\"any\").alias(\"total_orders\"), sum(\"order_value\").alias(\"total_value\"))\n",
    " .fillna(\"IS_NULL\", subset=\"client_id\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c4f12c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:35:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def process_table_analytics(batch_df, batch_id):\n",
    "  df_signal_rule_orders = change_data_feed_column_signal(batch_df, col, \"order_value\", \"order_value\")\n",
    "  df_signal_rule_count = change_data_feed_column_signal(df_signal_rule_orders, lit_1, \"any\", \"count_order\")\n",
    "\n",
    "  df_grouped = (df_signal_rule_count\n",
    "  .withColumn(\"year_month\", date_format(col(\"order_date\"), \"yyyy_MM\"))\n",
    "  .groupBy(\"year_month\", \"country_code\", \"client_id\")\n",
    "  .agg(sum(\"any\").alias(\"total_orders\"), sum(\"order_value\").alias(\"total_value\"))\n",
    "  .fillna(\"IS_NULL\", subset=\"client_id\"))\n",
    "\n",
    "  if spark.catalog.tableExists(orders_analyse):\n",
    "    set_columns = _write_table_update(df_grouped.drop(\"year_month\", \"country_code\", \"client_id\").columns)\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, orders_analyse)\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "      df_grouped.alias(\"source\"),\n",
    "          \"\"\"\n",
    "          target.year_month = source.year_month AND\n",
    "          target.country_code = source.country_code AND\n",
    "          target.client_id = source.client_id\n",
    "          \"\"\"\n",
    "      ).whenMatchedUpdate(\n",
    "          set = set_columns\n",
    "      ).whenNotMatchedInsertAll().execute()\n",
    "  else:\n",
    "    df_grouped.write.format(\"delta\").mode(\"append\").saveAsTable(orders_analyse)\n",
    "\n",
    "\n",
    "streaming = (spark\n",
    "             .readStream\n",
    "             .option(\"readChangeFeed\", \"true\")\n",
    "             .table(orders_table)\n",
    "             .writeStream\n",
    "             .format('delta')\n",
    "             .option(\"checkpointLocation\", orders_analyse_checkpoint)\n",
    "             .foreachBatch(process_table_analytics)\n",
    "             .trigger(availableNow=True)             \n",
    ")\n",
    "\n",
    "streaming.start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "171a5069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+------------+-----------+\n",
      "|year_month|country_code|client_id|total_orders|total_value|\n",
      "+----------+------------+---------+------------+-----------+\n",
      "|   2025_08|          BR|    CL002|           1|     299.99|\n",
      "|   2025_08|          US|    CL002|           1|     149.00|\n",
      "|   2025_08|          US|    CL009|           1|      99.99|\n",
      "|   2025_08|          US|    CL007|           1|     210.75|\n",
      "|   2025_08|          BR|    CL001|           2|     255.75|\n",
      "|   2025_08|          US|    CL006|           1|     100.00|\n",
      "|   2025_08|          BR|    CL011|           1|     300.00|\n",
      "|   2025_08|          BR|    CL003|           1|      80.00|\n",
      "|   2025_08|          BR|    CL004|           1|     560.00|\n",
      "|   2025_08|          BR|    CL005|           1|     215.00|\n",
      "|   2025_08|          US|    CL008|           1|      55.55|\n",
      "|   2025_08|          US|    CL010|           1|     150.00|\n",
      "+----------+------------+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(orders_analyse).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e89a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"ORD015\", None, Decimal(\"125.50\"), datetime(2025, 8, 3, 10, 30), \"BR\"),\n",
    "    (\"ORD016\", \"CL002\", Decimal(\"100.99\"), datetime(2025, 8, 2, 11, 15), \"BR\"),\n",
    "    (\"ORD017\", \"CL003\", Decimal(\"20.00\"), datetime(2025, 8, 5, 9, 45), \"BR\"),\n",
    "    (\"ORD018\", \"CL004\", Decimal(\"80.00\"), datetime(2025, 8, 9, 15, 10), \"BR\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "orders_df = spark.createDataFrame(data, schema=order_schema)\n",
    "\n",
    "# Write to Delta table\n",
    "orders_df.coalesce(1).write.format(\"delta\").mode(\"append\").saveAsTable(orders_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b89cc61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:36:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+------------+-----------+\n",
      "|year_month|country_code|client_id|total_orders|total_value|\n",
      "+----------+------------+---------+------------+-----------+\n",
      "|   2025_08|          BR|    CL002|           3|     501.97|\n",
      "|   2025_08|          BR|    CL003|           3|     120.00|\n",
      "|   2025_08|          BR|    CL004|           3|     720.00|\n",
      "|   2025_08|          BR|  IS_NULL|           2|     251.00|\n",
      "|   2025_08|          US|    CL002|           1|     149.00|\n",
      "|   2025_08|          US|    CL009|           1|      99.99|\n",
      "|   2025_08|          US|    CL007|           1|     210.75|\n",
      "|   2025_08|          BR|    CL001|           2|     255.75|\n",
      "|   2025_08|          US|    CL006|           1|     100.00|\n",
      "|   2025_08|          BR|    CL011|           1|     300.00|\n",
      "|   2025_08|          BR|    CL005|           1|     215.00|\n",
      "|   2025_08|          US|    CL008|           1|      55.55|\n",
      "|   2025_08|          US|    CL010|           1|     150.00|\n",
      "+----------+------------+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.start().awaitTermination()\n",
    "spark.table(orders_analyse).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50674efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:36:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+------------+-----------+\n",
      "|year_month|country_code|client_id|total_orders|total_value|\n",
      "+----------+------------+---------+------------+-----------+\n",
      "|   2025_08|          BR|    CL001|           4|     506.75|\n",
      "|   2025_08|          BR|    CL002|           3|     501.97|\n",
      "|   2025_08|          BR|    CL003|           3|     120.00|\n",
      "|   2025_08|          BR|    CL004|           3|     720.00|\n",
      "|   2025_08|          BR|  IS_NULL|           0|       0.00|\n",
      "|   2025_08|          US|    CL002|           1|     149.00|\n",
      "|   2025_08|          US|    CL009|           1|      99.99|\n",
      "|   2025_08|          US|    CL007|           1|     210.75|\n",
      "|   2025_08|          US|    CL006|           1|     100.00|\n",
      "|   2025_08|          BR|    CL011|           1|     300.00|\n",
      "|   2025_08|          BR|    CL005|           1|     215.00|\n",
      "|   2025_08|          US|    CL008|           1|      55.55|\n",
      "|   2025_08|          US|    CL010|           1|     150.00|\n",
      "+----------+------------+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"UPDATE {orders_table} SET client_id='CL001' WHERE order_id='ORD015'\")\n",
    "streaming.start().awaitTermination()\n",
    "spark.table(orders_analyse).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432e3ae",
   "metadata": {},
   "source": [
    "## Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39c7d25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|order_id|client_id|order_value|         order_date|country_code|    _change_type|_commit_version|   _commit_timestamp|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "|  ORD006|    CL005|     199.99|2025-08-03 16:30:00|          US|          delete|              2|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      75.00|2025-08-02 09:45:00|          BR| update_preimage|              3|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      80.00|2025-08-05 09:00:00|          BR|update_postimage|              3|2025-08-12 18:33:...|\n",
      "|  ORD006|    CL005|     215.00|2025-08-05 10:00:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD011|    CL009|      99.99|2025-08-05 11:30:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD012|    CL010|     150.00|2025-08-05 12:45:00|          US|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD013|    CL011|     300.00|2025-08-05 13:15:00|          BR|          insert|              3|2025-08-12 18:33:...|\n",
      "|  ORD001|     NULL|     125.50|2025-08-01 10:30:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD002|    CL002|     299.99|2025-08-01 11:15:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD003|    CL003|      75.00|2025-08-02 09:45:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD004|    CL004|     560.00|2025-08-02 15:10:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD005|    CL001|     130.25|2025-08-03 14:00:00|          BR|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD006|    CL005|     199.99|2025-08-03 16:30:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD007|    CL006|      89.95|2025-08-04 12:45:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD008|    CL002|     149.00|2025-08-04 17:25:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD009|    CL007|     210.75|2025-08-04 18:50:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "|  ORD010|    CL008|      55.55|2025-08-04 19:05:00|          US|          insert|              1|2025-08-12 18:33:...|\n",
      "+--------+---------+-----------+-------------------+------------+----------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 0) \\\n",
    "  .option(\"endingVersion\", 3) \\\n",
    "  .table(orders_table)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93f63c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 12 files and directories in a total of 2 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"VACUUM {orders_table} RETAIN 0 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "985d6007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 18:40:42 ERROR Executor: Exception in task 0.0 in stage 423.0 (TID 14494)\n",
      "org.apache.spark.SparkFileNotFoundException: File file:/home/jacques/Documents/courses/DatabricksCourse/tips/spark-warehouse/orders/_change_data/cdc-00000-b22b1601-873c-443a-bbcf-854ffe476400.c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/08/12 18:40:42 WARN TaskSetManager: Lost task 0.0 in stage 423.0 (TID 14494) (192.168.1.23 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/jacques/Documents/courses/DatabricksCourse/tips/spark-warehouse/orders/_change_data/cdc-00000-b22b1601-873c-443a-bbcf-854ffe476400.c000.snappy.parquet does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/08/12 18:40:42 ERROR TaskSetManager: Task 0 in stage 423.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1183.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 423.0 failed 1 times, most recent failure: Lost task 0.0 in stage 423.0 (TID 14494) (192.168.1.23 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/jacques/Documents/courses/DatabricksCourse/tips/spark-warehouse/orders/_change_data/cdc-00000-b22b1601-873c-443a-bbcf-854ffe476400.c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.GeneratedMethodAccessor244.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/home/jacques/Documents/courses/DatabricksCourse/tips/spark-warehouse/orders/_change_data/cdc-00000-b22b1601-873c-443a-bbcf-854ffe476400.c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/courses/DatabricksCourse/venv/lib/python3.13/site-packages/pyspark/sql/dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/courses/DatabricksCourse/venv/lib/python3.13/site-packages/pyspark/sql/dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/courses/DatabricksCourse/venv/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/courses/DatabricksCourse/venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/courses/DatabricksCourse/venv/lib/python3.13/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1183.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 423.0 failed 1 times, most recent failure: Lost task 0.0 in stage 423.0 (TID 14494) (192.168.1.23 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/jacques/Documents/courses/DatabricksCourse/tips/spark-warehouse/orders/_change_data/cdc-00000-b22b1601-873c-443a-bbcf-854ffe476400.c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.GeneratedMethodAccessor244.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/home/jacques/Documents/courses/DatabricksCourse/tips/spark-warehouse/orders/_change_data/cdc-00000-b22b1601-873c-443a-bbcf-854ffe476400.c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:222)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
